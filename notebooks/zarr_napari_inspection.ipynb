{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df6f5fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import random\n",
    "import napari\n",
    "import numpy as np\n",
    "import torch\n",
    "from skimage.segmentation import relabel_sequential\n",
    "from scipy.ndimage import distance_transform_edt, map_coordinates\n",
    "from matplotlib import gridspec, ticker\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.nn as nn\n",
    "from torchvision.io import decode_image \n",
    "import torchvision.transforms.v2 as transforms_v2\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import os\n",
    "os.environ[\"DISPLAY\"] = \":1001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29906a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AD', 'CBD', 'PSP', 'PiD']\n",
      "['FTL_PiD_NA16-197_XY03.tif', 'FTL_PiD_NA22-053_XY09.tif', 'FTL_PiD_NA22-109_XY06.tif', 'GSK3a_PiD_NA16-197_XY02.tif', 'GSK3a_PiD_NA22-053_XY04.tif', 'GSK3a_PiD_NA22-109_XY03.tif', 'GSK3a_PiD_NA22-109_XY04.tif', 'LAMP2_PiD_NA16-197_XY01.tif', 'LAMP2_PiD_NA22-053_XY01.tif', 'LAMP2_PiD_NA22-109_XY01.tif', 'LAMP2_PiD_NA22-109_XY02.tif', 'LAMP2_PiD_NA22-109_XY05.tif', 'VGF_PiD_NA16-197_XY02.tif', 'VGF_PiD_NA16-197_XY05.tif', 'VGF_PiD_NA22-053_XY01.tif', 'VGF_PiD_NA22-109_XY01.tif', 'VGF_PiD_NA22-109_XY05.tif', 'VPS35_PiD_NA16-197_XY01.tif', 'VPS35_PiD_NA22-053_XY02.tif', 'VPS35_PiD_NA22-109_XY02.tif']\n",
      "(2729, 3638)\n",
      "(11, 2729, 3638)\n",
      "(48, 512, 512)\n",
      "(48, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "zarr_path = \"/mnt/efs/aimbl_2025/student_data/S-DM/Data/zarr_storage/tauopathies.zarr\"\n",
    "root = zarr.open (zarr_path)\n",
    "level_1 = sorted (list (root.keys()))\n",
    "print (level_1)\n",
    "index_1 = random.choice (range (0, len (level_1)))\n",
    "level_2 = sorted (list(root[level_1[index_1]].keys()))\n",
    "print (level_2)\n",
    "index_2 = random.choice (range (0, len (level_2)))\n",
    "\n",
    "\n",
    "image = root[level_1[index_1]][level_2[index_2]][\"x\"]\n",
    "segmentation = root[level_1[index_1]][level_2[index_2]][\"y\"]\n",
    "\n",
    "print (image.shape)\n",
    "print (segmentation.shape)\n",
    "\n",
    "image_stack = root[level_1[index_1]][level_2[index_2]][\"x_cropped\"]\n",
    "segmentation_stack = root[level_1[index_1]][level_2[index_2]][\"y_cropped\"]\n",
    "\n",
    "print (image_stack.shape)\n",
    "print (segmentation_stack.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e89e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "223825ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewer.add_image(image_stack, name=\"raw\")\n",
    "# viewer.add_labels(segmentation_stack, name=\"seg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9878398",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_data (zarr_path):\n",
    "    x_arrays = []\n",
    "    y_arrays = []\n",
    "    root = zarr.open (zarr_path)\n",
    "    for conditions in list (root.keys()):\n",
    "        images = root [conditions].keys()\n",
    "        for fov in images:\n",
    "            x = root[conditions][fov][\"x_cropped\"][:]\n",
    "            y = root[conditions][fov][\"y_cropped\"][:].astype (\"int16\")\n",
    "            #y1 = root[conditions][fov][\"y_cropped\"][:].astype (\"int64\")\n",
    "            #assert (y == y1).all()\n",
    "            # print (x.dtype, y.dtype)\n",
    "            x_arrays.append (x)\n",
    "            y_arrays.append (y)\n",
    "    x_array = np.concatenate (x_arrays)\n",
    "    y_array = np.concatenate (y_arrays)\n",
    "    return x_array, y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc790694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sdt(labels: np.ndarray, scale: int = 5):\n",
    "    \"\"\"Function to compute a signed distance transform.\"\"\"\n",
    "    dims = len(labels.shape)\n",
    "    # Create a placeholder array of infinite distances\n",
    "    distances = np.ones(labels.shape, dtype=np.float32) * np.inf\n",
    "    for axis in range(dims):\n",
    "        # Here we compute the boundaries by shifting the labels and comparing to the original labels\n",
    "        # This can be visualized in 1D as:\n",
    "        # a a a b b c c c\n",
    "        #   a a a b b c c c\n",
    "        #   1 1 0 1 0 1 1\n",
    "        # Applying a half pixel shift makes the result more obvious:\n",
    "        # a a a b b c c c\n",
    "        #  1 1 0 1 0 1 1\n",
    "        bounds = (\n",
    "            labels[*[slice(None) if a != axis else slice(1, None) for a in range(dims)]]\n",
    "            == labels[\n",
    "                *[slice(None) if a != axis else slice(None, -1) for a in range(dims)]\n",
    "            ]\n",
    "        )\n",
    "        # pad to account for the lost pixel\n",
    "        bounds = np.pad(\n",
    "            bounds,\n",
    "            [(1, 1) if a == axis else (0, 0) for a in range(dims)],\n",
    "            mode=\"constant\",\n",
    "            constant_values=1,\n",
    "        )\n",
    "        # compute distances on the boundary mask\n",
    "        axis_distances = distance_transform_edt(bounds)\n",
    "\n",
    "        # compute the coordinates of each original pixel relative to the boundary mask and distance transform.\n",
    "        # Its just a half pixel shift in the axis we computed boundaries for.\n",
    "        coordinates = np.meshgrid(\n",
    "            *[\n",
    "                (\n",
    "                    range(axis_distances.shape[a])\n",
    "                    if a != axis\n",
    "                    else np.linspace(\n",
    "                        0.5, axis_distances.shape[a] - 1.5, labels.shape[a]\n",
    "                    )\n",
    "                )\n",
    "                for a in range(dims)\n",
    "            ],\n",
    "            indexing=\"ij\",\n",
    "        )\n",
    "        coordinates = np.stack(coordinates)\n",
    "\n",
    "        # Interpolate the distances to the original pixel coordinates\n",
    "        sampled = map_coordinates(\n",
    "            axis_distances,\n",
    "            coordinates=coordinates,\n",
    "            order=3,\n",
    "        )\n",
    "\n",
    "        # Update the distances with the minimum distance to a boundary in this axis\n",
    "        distances = np.minimum(distances, sampled)\n",
    "\n",
    "    # Normalize the distances to be between -1 and 1\n",
    "    distances = np.tanh(distances / scale)\n",
    "\n",
    "    # Invert the distances for pixels in the background\n",
    "    distances[labels == 0] *= -1\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22f9da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropDataset(Dataset):\n",
    "    def __init__(self, zarr_path, transform = None, img_transform = None):\n",
    "        \n",
    "        \n",
    "        self.zarr_path = zarr_path\n",
    "\n",
    "        self.x, self.y = load_data(self.zarr_path)\n",
    "        self.transform = transform\n",
    "        self.img_transform = img_transform\n",
    "        \n",
    "       \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.x[idx]  \n",
    "        seg = self.y[idx] \n",
    "\n",
    "        img = torch.tensor(img).unsqueeze(0)\n",
    "        seg = torch.tensor(seg).unsqueeze(0)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform (img)\n",
    "            seg = self.transform(seg)\n",
    "        \n",
    "        if self.img_transform:\n",
    "            img = self.img_transform(img)\n",
    "\n",
    "        sdt = compute_sdt (seg) \n",
    "\n",
    "    \n",
    "\n",
    "        return torch.tensor(img, dtype=torch.float32), torch.tensor(sdt, dtype=torch.float32), seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fc66175",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataset = CropDataset (zarr_path=zarr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa71e6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_489611/1863317717.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(img, dtype=torch.float32), torch.tensor(sdt, dtype=torch.float32), seg\n"
     ]
    }
   ],
   "source": [
    "index = random.choice (range(0, len (dataset)))\n",
    "img = dataset [index][0].squeeze(0)\n",
    "sdt = dataset[index][1].squeeze (0)\n",
    "seg = dataset[index][2].squeeze (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f49a1d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'seg [3]' at 0x74af268f04d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer.add_image(img, name=\"raw\")\n",
    "viewer.add_image(sdt, name=\"sdt\")\n",
    "viewer.add_labels(seg, name=\"seg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masketeers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
